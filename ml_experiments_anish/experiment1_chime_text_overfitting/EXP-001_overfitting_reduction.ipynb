{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55064e1b",
   "metadata": {},
   "source": [
    "# EXP-001 — Reducing Overfitting in CHIME Text Classification\n",
    "\n",
    "This notebook runs a controlled experiment to reduce overfitting in CHIME framework text classification using BERT-based models. It compares three runs: a baseline BERT, a regularized BERT with early stopping and weight decay, and a smaller DistilBERT model. Outputs include configuration files, metrics, confusion matrices, learning curves, and optional test predictions.\n",
    "\n",
    "\n",
    "**Expected runtime:** depends on CPU/GPU. On CPU, BERT may take several minutes per run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df1bd5",
   "metadata": {},
   "source": [
    "## Cell 1 — Setup & dependency check\n",
    "This notebook expects `torch`, `transformers`, `datasets`, `pandas`, `scikit-learn`, `matplotlib`, `seaborn`.\n",
    "\n",
    "**Expected output:** a quick confirmation of versions and whether CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f104abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from dataclasses import asdict, dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "print('torch:', torch.__version__)\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('device:', 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b95f91",
   "metadata": {},
   "source": [
    "## Cell 2 — Experiment constants\n",
    "Adjust these if needed. Keeping them centralized makes runs consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae3feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Dataset source (Option A)\n",
    "HF_DATASET_NAME = 'ashh007/DREAMS-CHIME-dataset'\n",
    "\n",
    "# Models to compare\n",
    "RUNS = [\n",
    "    {\n",
    "        'run_name': 'A_bert_baseline',\n",
    "        'model_ckpt': 'bert-base-uncased',\n",
    "        'use_early_stopping': False,\n",
    "        'weight_decay': 0.0,\n",
    "        'dropout': None,\n",
    "    },\n",
    "    {\n",
    "        'run_name': 'B_bert_regularized',\n",
    "        'model_ckpt': 'bert-base-uncased',\n",
    "        'use_early_stopping': True,\n",
    "        'weight_decay': 0.01,\n",
    "        # Optional: increase dropout slightly to fight overfitting\n",
    "        'dropout': 0.2,\n",
    "    },\n",
    "    {\n",
    "        'run_name': 'C_distilbert_regularized',\n",
    "        'model_ckpt': 'distilbert-base-uncased',\n",
    "        'use_early_stopping': True,\n",
    "        'weight_decay': 0.01,\n",
    "        'dropout': None,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Reproducibility controls (kept lightweight)\n",
    "# NOTE: we separate split randomness from training randomness so you can\n",
    "# re-run training with different seeds on the *same* train/val/test split.\n",
    "SPLIT_SEED = 42\n",
    "SEED = 42\n",
    "MAX_LENGTH = 128\n",
    "TEST_SIZE = 0.10\n",
    "VAL_SIZE = 0.10\n",
    "\n",
    "# Training knobs (CPU-friendly defaults)\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "LR = 2e-5\n",
    "\n",
    "# Output folder\n",
    "BASE_OUTPUT_DIR = os.path.join('ml_experiments_anish', 'experiment1_chime_text_overfitting', 'runs')\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "print('Split seed:', SPLIT_SEED)\n",
    "print('Training seed set to:', SEED)\n",
    "print('Outputs will be saved under:', BASE_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d01e0d",
   "metadata": {},
   "source": [
    "# Cell 3 - Filesystem diagonis\n",
    "Confirms the runtime environment, especially usefull when using Colab VMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0cc4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print('=== Runtime filesystem diagnostic ===')\n",
    "print('Platform:', platform.platform())\n",
    "print('Python:', sys.version.split()[0])\n",
    "print('CWD:', os.getcwd())\n",
    "print('BASE_OUTPUT_DIR (as set):', BASE_OUTPUT_DIR)\n",
    "print('BASE_OUTPUT_DIR (absolute):', str(Path(BASE_OUTPUT_DIR).resolve()))\n",
    "\n",
    "# If you are using a Colab kernel, files are being written to the Colab VM filesystem,\n",
    "# not your local VS Code workspace. This cell helps you confirm where artifacts live.\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "print('Detected Colab kernel:', in_colab)\n",
    "\n",
    "try:\n",
    "    p = Path(BASE_OUTPUT_DIR)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    entries = sorted([x.name for x in p.iterdir()])\n",
    "    print(f'Entries in BASE_OUTPUT_DIR ({len(entries)}):', entries[:50])\n",
    "except Exception as e:\n",
    "    print('Could not list BASE_OUTPUT_DIR due to:', repr(e))\n",
    "\n",
    "# Write a tiny probe file so you can confirm persistence in the active kernel filesystem.\n",
    "try:\n",
    "    probe = Path(BASE_OUTPUT_DIR) / '_write_probe.txt'\n",
    "    probe.write_text('ok\\n', encoding='utf-8')\n",
    "    print('Wrote probe file:', str(probe))\n",
    "except Exception as e:\n",
    "    print('Could not write probe file due to:', repr(e))\n",
    "\n",
    "print('Tip: if you want artifacts to persist outside the Colab VM, set BASE_OUTPUT_DIR to a Google Drive path (after mounting Drive).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477d6c3",
   "metadata": {},
   "source": [
    "# Cell 4 - Checks disk usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5329836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print('=== Disk usage (Colab VM) ===')\n",
    "for p in ['/', '/content', str(Path(BASE_OUTPUT_DIR).resolve())]:\n",
    "    try:\n",
    "        du = shutil.disk_usage(p)\n",
    "        free_gb = du.free / (1024**3)\n",
    "        total_gb = du.total / (1024**3)\n",
    "        used_gb = du.used / (1024**3)\n",
    "        print(f'{p:>30}  free={free_gb:6.2f} GB  used={used_gb:6.2f} GB  total={total_gb:6.2f} GB')\n",
    "    except Exception as e:\n",
    "        print(f'{p:>30}  (error: {e!r})')\n",
    "\n",
    "# ---- Optional cleanup helpers ----\n",
    "# When the Colab VM runs out of space, you can delete old run folders and/or HF caches.\n",
    "# This does NOT affect your local Windows repo unless you explicitly copy things back.\n",
    "\n",
    "DO_CLEANUP = True  # set True to actually delete files\n",
    "KEEP_BASELINE_RUNS = False  # set False to also delete A/B/C runs\n",
    "KEEP_LAST_N_RUN_FOLDERS = 0  # keep the newest N run folders (after keeping baseline)\n",
    "DELETE_SWEEP_RUNS_FIRST = True  # delete only sweep runs (S_bert_*) before deleting other runs\n",
    "DELETE_HF_CACHE = False  # last resort: redownloads models/datasets later\n",
    "\n",
    "# Common HF cache locations on Colab\n",
    "HF_CACHE_DIRS = [\n",
    "    Path.home() / '.cache' / 'huggingface' / 'hub',\n",
    "    Path.home() / '.cache' / 'huggingface' / 'datasets',\n",
    "    Path('/root/.cache/huggingface/hub'),\n",
    "    Path('/root/.cache/huggingface/datasets'),\n",
    "    Path('/root/.cache/torch'),\n",
    "    Path.home() / '.cache' / 'torch',\n",
    "]\n",
    "\n",
    "def _is_run_dir(p: Path) -> bool:\n",
    "    if not p.is_dir():\n",
    "        return False\n",
    "    name = p.name\n",
    "    return ('_seed' in name) and (name[0].isdigit())\n",
    "\n",
    "def _is_sweep_run_dir(p: Path) -> bool:\n",
    "    return _is_run_dir(p) and ('_S_' in p.name or 'S_bert_' in p.name)\n",
    "\n",
    "def _is_baseline_run_dir(p: Path) -> bool:\n",
    "    if not _is_run_dir(p):\n",
    "        return False\n",
    "    return (\n",
    "        ('A_bert_baseline' in p.name)\n",
    "        or ('B_bert_regularized' in p.name)\n",
    "        or ('C_distilbert_regularized' in p.name)\n",
    "    )\n",
    "\n",
    "def _delete_path(p: Path):\n",
    "    if p.is_dir():\n",
    "        shutil.rmtree(p, ignore_errors=True)\n",
    "    elif p.exists():\n",
    "        try:\n",
    "            p.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def cleanup_disk():\n",
    "    base = Path(BASE_OUTPUT_DIR)\n",
    "    if not base.exists():\n",
    "        print('BASE_OUTPUT_DIR does not exist:', str(base))\n",
    "        return\n",
    "\n",
    "    # Collect run folders and sort oldest->newest by name (timestamp prefix sorts lexicographically)\n",
    "    run_dirs = sorted([p for p in base.iterdir() if _is_run_dir(p)], key=lambda p: p.name)\n",
    "    if not run_dirs:\n",
    "        print('No run folders found under:', str(base))\n",
    "        return\n",
    "\n",
    "    kept = set()\n",
    "    if KEEP_BASELINE_RUNS:\n",
    "        for p in run_dirs:\n",
    "            if _is_baseline_run_dir(p):\n",
    "                kept.add(p)\n",
    "\n",
    "    # Optionally delete sweep runs first (usually the bulk)\n",
    "    deleted = []\n",
    "    if DELETE_SWEEP_RUNS_FIRST:\n",
    "        for p in list(run_dirs):\n",
    "            if p in kept:\n",
    "                continue\n",
    "            if _is_sweep_run_dir(p):\n",
    "                deleted.append(p)\n",
    "                _delete_path(p)\n",
    "\n",
    "        # Refresh list after deletions\n",
    "        run_dirs = sorted([p for p in base.iterdir() if _is_run_dir(p)], key=lambda p: p.name)\n",
    "\n",
    "    # Keep newest N (plus kept baselines)\n",
    "    remaining = [p for p in run_dirs if p not in kept]\n",
    "    to_keep_newest = set(remaining[-KEEP_LAST_N_RUN_FOLDERS:]) if KEEP_LAST_N_RUN_FOLDERS else set()\n",
    "    for p in remaining:\n",
    "        if p in to_keep_newest:\n",
    "            continue\n",
    "        deleted.append(p)\n",
    "        _delete_path(p)\n",
    "\n",
    "    print(f'Deleted {len(deleted)} run folders.')\n",
    "    if deleted:\n",
    "        print('Examples deleted:', [d.name for d in deleted[:5]])\n",
    "    print('Kept baseline runs:', [p.name for p in sorted(kept, key=lambda x: x.name)])\n",
    "    print('Kept newest runs:', [p.name for p in sorted(to_keep_newest, key=lambda x: x.name)])\n",
    "\n",
    "    if DELETE_HF_CACHE:\n",
    "        cache_deleted = 0\n",
    "        for d in HF_CACHE_DIRS:\n",
    "            if d.exists():\n",
    "                _delete_path(d)\n",
    "                cache_deleted += 1\n",
    "        print(f'Deleted {cache_deleted} HF/torch cache dirs (if existed).')\n",
    "\n",
    "    # Print disk usage after cleanup\n",
    "    du2 = shutil.disk_usage('/')\n",
    "    print(f\"Free space after cleanup: {du2.free / (1024**3):.2f} GB\")\n",
    "\n",
    "if DO_CLEANUP:\n",
    "    cleanup_disk()\n",
    "else:\n",
    "    print('Cleanup is disabled. Set DO_CLEANUP=True to delete old artifacts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ccc4b",
   "metadata": {},
   "source": [
    "## Cell 4 — Load dataset from Hugging Face\n",
    "\n",
    "**Expected output:** dataset columns, row count, and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2ebe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(HF_DATASET_NAME)\n",
    "print(ds)\n",
    "\n",
    "# Convert to a single pandas DataFrame for consistent splitting\n",
    "# Handles cases where dataset has a single split or multiple splits.\n",
    "if hasattr(ds, 'keys') and len(ds.keys()) > 0:\n",
    "    # Prefer a 'train' split if it exists, otherwise take the first split\n",
    "    split_name = 'train' if 'train' in ds else list(ds.keys())[0]\n",
    "    base = ds[split_name]\n",
    "else:\n",
    "    base = ds\n",
    "\n",
    "df = base.to_pandas()\n",
    "print('Rows:', len(df))\n",
    "print('Columns:', list(df.columns))\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace66d7",
   "metadata": {},
   "source": [
    "## Cell 5 — Normalize columns (text + label)\n",
    "The legacy EXP-001 notebook used `CAPTIONS` and `labels`. Here we detect likely column names and standardize them to:\n",
    "- `text`\n",
    "- `label`\n",
    "\n",
    "**Expected output:** confirmation of chosen columns + label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7aa2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_first_existing(columns, candidates):\n",
    "    for c in candidates:\n",
    "        if c in columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "text_col = pick_first_existing(df.columns, ['CAPTIONS', 'caption', 'captions', 'text', 'sentence', 'content'])\n",
    "label_col = pick_first_existing(df.columns, ['labels', 'label', 'category', 'class', 'target'])\n",
    "\n",
    "if text_col is None or label_col is None:\n",
    "    raise ValueError(\n",
    "        f'Could not infer text/label columns. Found columns={list(df.columns)}. '\n",
    "        'Please update the candidates list.'\n",
    "    )\n",
    "\n",
    "df = df[[text_col, label_col]].rename(columns={text_col: 'text', label_col: 'label'})\n",
    "df['text'] = df['text'].astype(str).fillna('')\n",
    "df['label'] = df['label'].astype(str)\n",
    "\n",
    "# Drop empty texts (if any)\n",
    "df = df[df['text'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "print('Using text column:', text_col)\n",
    "print('Using label column:', label_col)\n",
    "print('Rows after cleanup:', len(df))\n",
    "\n",
    "label_counts = df['label'].value_counts()\n",
    "display(label_counts)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.title('Label distribution')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e4d0f",
   "metadata": {},
   "source": [
    "## Dataset duplicate / near-duplicate audit\n",
    "This checks for **exact duplicates** and **near-duplicates** in:\n",
    "- the in-memory `df` loaded from Hugging Face, and\n",
    "- the local CSV `ml_experiments_anish/DREAMS_CHIME_dataset.csv` (if present).\n",
    "\n",
    "High duplicate rates can inflate train/val/test metrics because similar texts can land in different splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d62ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def _normalize_text_simple(t: str) -> str:\n",
    "    import re\n",
    "    t = str(t).lower().strip()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z0-9\\s]\", \"\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def audit_duplicates_in_df(df_in: pd.DataFrame, text_col: str = 'text', label_col: str | None = 'label', thr: float = 0.92):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    tmp = df_in.copy()\n",
    "    tmp[text_col] = tmp[text_col].astype(str).fillna('')\n",
    "    tmp['__norm__'] = tmp[text_col].map(_normalize_text_simple)\n",
    "\n",
    "    raw_dup_rows = int(tmp[text_col].duplicated(keep=False).sum())\n",
    "    norm_dup_rows = int(tmp['__norm__'].duplicated(keep=False).sum())\n",
    "    print('Rows:', len(tmp))\n",
    "    print('Exact duplicate rows (raw):', raw_dup_rows)\n",
    "    print('Exact duplicate rows (normalized):', norm_dup_rows)\n",
    "\n",
    "    # show biggest normalized dup groups\n",
    "    if norm_dup_rows > 0:\n",
    "        g = (\n",
    "            tmp.groupby('__norm__')\n",
    "            .size()\n",
    "            .sort_values(ascending=False)\n",
    "            .reset_index(name='count')\n",
    "        )\n",
    "        g = g[g['count'] >= 2]\n",
    "        print('\\nTop normalized duplicate groups:')\n",
    "        display(g.head(10))\n",
    "\n",
    "    # near-duplicates\n",
    "    nonempty = tmp['__norm__'].str.len() > 0\n",
    "    texts = tmp.loc[nonempty, '__norm__'].tolist()\n",
    "    idx_map = np.flatnonzero(nonempty.to_numpy())\n",
    "    near_pairs = []\n",
    "    if len(texts) >= 2:\n",
    "        vec = TfidfVectorizer(min_df=2, ngram_range=(3, 5), analyzer='char_wb')\n",
    "        X = vec.fit_transform(texts)\n",
    "        n_neighbors = min(6, len(texts))\n",
    "        nn = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm='brute').fit(X)\n",
    "        dists, idxs = nn.kneighbors(X)\n",
    "        seen = set()\n",
    "        for i in range(len(texts)):\n",
    "            for jpos in range(1, idxs.shape[1]):\n",
    "                j = int(idxs[i, jpos])\n",
    "                a = int(idx_map[i]); b = int(idx_map[j])\n",
    "                lo, hi = (a, b) if a < b else (b, a)\n",
    "                if (lo, hi) in seen:\n",
    "                    continue\n",
    "                sim = 1.0 - float(dists[i, jpos])\n",
    "                if sim < thr:\n",
    "                    continue\n",
    "                if tmp.loc[lo, '__norm__'] == tmp.loc[hi, '__norm__']:\n",
    "                    continue\n",
    "                seen.add((lo, hi))\n",
    "                item = {\n",
    "                    'row_a': lo,\n",
    "                    'row_b': hi,\n",
    "                    'similarity': sim,\n",
    "                    'text_a': tmp.loc[lo, text_col][:200],\n",
    "                    'text_b': tmp.loc[hi, text_col][:200],\n",
    "                }\n",
    "                if label_col and label_col in tmp.columns:\n",
    "                    item['label_a'] = tmp.loc[lo, label_col]\n",
    "                    item['label_b'] = tmp.loc[hi, label_col]\n",
    "                near_pairs.append(item)\n",
    "        near_pairs.sort(key=lambda x: -x['similarity'])\n",
    "\n",
    "    print(f\"\\nNear-duplicate pairs (similarity >= {thr}):\", len(near_pairs))\n",
    "    if near_pairs:\n",
    "        display(pd.DataFrame(near_pairs[:10]))\n",
    "    return {'raw_dup_rows': raw_dup_rows, 'norm_dup_rows': norm_dup_rows, 'near_pairs': near_pairs}\n",
    "\n",
    "print('=== Audit: HF-loaded df (after column normalization) ===')\n",
    "_ = audit_duplicates_in_df(df, text_col='text', label_col='label', thr=0.92)\n",
    "\n",
    "local_csv = Path('ml_experiments_anish') / 'DREAMS_CHIME_dataset.csv'\n",
    "if local_csv.exists():\n",
    "    print('\\n=== Audit: local CSV (ml_experiments_anish/DREAMS_CHIME_dataset.csv) ===')\n",
    "    local_df = pd.read_csv(local_csv)\n",
    "    # map expected columns\n",
    "    tcol = 'CAPTIONS' if 'CAPTIONS' in local_df.columns else ('text' if 'text' in local_df.columns else local_df.columns[0])\n",
    "    lcol = 'labels' if 'labels' in local_df.columns else (None)\n",
    "    local_df = local_df.rename(columns={tcol: 'text', **({lcol: 'label'} if lcol else {})})\n",
    "    _ = audit_duplicates_in_df(local_df, text_col='text', label_col=('label' if 'label' in local_df.columns else None), thr=0.92)\n",
    "else:\n",
    "    print('\\nLocal CSV not found at:', str(local_csv.resolve()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffe2a9",
   "metadata": {},
   "source": [
    "## Build duplicate/near-duplicate groups (`group_id`)\n",
    "This assigns each row a `group_id` so that **near-duplicate text variants are treated as one group**.\n",
    "We will use `group_id` in the split step so groups can’t leak across train/val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9505a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "USE_GROUP_SPLIT = True\n",
    "GROUP_SIM_THRESHOLD = 0.92  # same threshold you used in the audit\n",
    "GROUP_N_NEIGHBORS = 6\n",
    "\n",
    "def _norm_for_grouping(t: str) -> str:\n",
    "    t = str(t).lower().strip()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z0-9\\s]\", \"\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "class _UnionFind:\n",
    "    def __init__(self, n: int):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "    def find(self, x: int) -> int:\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a: int, b: int) -> None:\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb:\n",
    "            return\n",
    "        if self.rank[ra] < self.rank[rb]:\n",
    "            self.parent[ra] = rb\n",
    "        elif self.rank[ra] > self.rank[rb]:\n",
    "            self.parent[rb] = ra\n",
    "        else:\n",
    "            self.parent[rb] = ra\n",
    "            self.rank[ra] += 1\n",
    "\n",
    "def build_group_ids(texts: pd.Series, thr: float = 0.92, n_neighbors: int = 6) -> np.ndarray:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    raw = texts.astype(str).fillna('')\n",
    "    norm = raw.map(_norm_for_grouping)\n",
    "    n = len(norm)\n",
    "    uf = _UnionFind(n)\n",
    "\n",
    "    # 1) Union exact duplicates by normalized text\n",
    "    buckets = {}\n",
    "    for i, key in enumerate(norm.tolist()):\n",
    "        if not key:\n",
    "            continue\n",
    "        if key in buckets:\n",
    "            uf.union(i, buckets[key])\n",
    "        else:\n",
    "            buckets[key] = i\n",
    "\n",
    "    # 2) Union near-duplicates using char-ngrams + cosine similarity\n",
    "    nonempty_mask = norm.str.len() > 0\n",
    "    texts_nonempty = norm.loc[nonempty_mask].tolist()\n",
    "    idx_map = np.flatnonzero(nonempty_mask.to_numpy())\n",
    "    if len(texts_nonempty) >= 2:\n",
    "        vec = TfidfVectorizer(min_df=2, ngram_range=(3, 5), analyzer='char_wb')\n",
    "        X = vec.fit_transform(texts_nonempty)\n",
    "        k = min(int(n_neighbors), len(texts_nonempty))\n",
    "        nn = NearestNeighbors(n_neighbors=k, metric='cosine', algorithm='brute').fit(X)\n",
    "        dists, idxs = nn.kneighbors(X)\n",
    "        for i in range(len(texts_nonempty)):\n",
    "            for jpos in range(1, idxs.shape[1]):\n",
    "                j = int(idxs[i, jpos])\n",
    "                sim = 1.0 - float(dists[i, jpos])\n",
    "                if sim < thr:\n",
    "                    continue\n",
    "                a = int(idx_map[i])\n",
    "                b = int(idx_map[j])\n",
    "                uf.union(a, b)\n",
    "\n",
    "    roots = np.array([uf.find(i) for i in range(n)], dtype=int)\n",
    "    # compress roots into 0..G-1 ids for readability\n",
    "    unique_roots, group_ids = np.unique(roots, return_inverse=True)\n",
    "    return group_ids\n",
    "\n",
    "# Attach group ids to df (used by split)\n",
    "df = df.copy()\n",
    "df['group_id'] = build_group_ids(df['text'], thr=GROUP_SIM_THRESHOLD, n_neighbors=GROUP_N_NEIGHBORS)\n",
    "\n",
    "group_sizes = df['group_id'].value_counts()\n",
    "print('USE_GROUP_SPLIT:', USE_GROUP_SPLIT)\n",
    "print('Groups:', int(group_sizes.shape[0]), 'Rows:', len(df))\n",
    "print('Largest group size:', int(group_sizes.max()))\n",
    "print('Groups with size>=2:', int((group_sizes >= 2).sum()))\n",
    "print('Rows in groups size>=2:', int(group_sizes[group_sizes >= 2].sum()))\n",
    "display(group_sizes.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d71791",
   "metadata": {},
   "source": [
    "## Cell 6 — Train/Val/Test split\n",
    "Creates consistent splits with stratification.\n",
    "\n",
    "**Expected output:** split sizes and per-split label distribution sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fd3dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group-aware split (recommended): prevents duplicate/near-duplicate leakage\n",
    "if bool(globals().get('USE_GROUP_SPLIT', False)) and ('group_id' in df.columns):\n",
    "    try:\n",
    "        from sklearn.model_selection import StratifiedGroupKFold\n",
    "        y = df['label']\n",
    "        groups = df['group_id']\n",
    "        n_splits_test = max(2, int(round(1.0 / float(TEST_SIZE))))\n",
    "        sgkf_test = StratifiedGroupKFold(n_splits=n_splits_test, shuffle=True, random_state=SPLIT_SEED)\n",
    "        train_val_idx, test_idx = next(sgkf_test.split(df, y=y, groups=groups))\n",
    "        train_val_df = df.iloc[train_val_idx].reset_index(drop=True)\n",
    "        test_df = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "        val_relative = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "        n_splits_val = max(2, int(round(1.0 / float(val_relative))))\n",
    "        sgkf_val = StratifiedGroupKFold(n_splits=n_splits_val, shuffle=True, random_state=SPLIT_SEED)\n",
    "        y_tv = train_val_df['label']\n",
    "        g_tv = train_val_df['group_id']\n",
    "        train_idx, val_idx = next(sgkf_val.split(train_val_df, y=y_tv, groups=g_tv))\n",
    "        train_df = train_val_df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = train_val_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        # Safety: ensure no group leaks across splits\n",
    "        tr_g = set(train_df['group_id'].tolist())\n",
    "        va_g = set(val_df['group_id'].tolist())\n",
    "        te_g = set(test_df['group_id'].tolist())\n",
    "        assert tr_g.isdisjoint(va_g) and tr_g.isdisjoint(te_g) and va_g.isdisjoint(te_g)\n",
    "        print('Used StratifiedGroupKFold (leakage-safe).')\n",
    "    except Exception as e:\n",
    "        print('Group-aware split requested, but failed; falling back to random stratified split. Error:', repr(e))\n",
    "        USE_GROUP_SPLIT = False\n",
    "\n",
    "# Fallback: classic stratified random split\n",
    "if not bool(globals().get('USE_GROUP_SPLIT', False)) or ('group_id' not in df.columns):\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=SPLIT_SEED,\n",
    "        stratify=df['label'],\n",
    "    )\n",
    "\n",
    "    val_relative = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=val_relative,\n",
    "        random_state=SPLIT_SEED,\n",
    "        stratify=train_val_df['label'],\n",
    "    )\n",
    "\n",
    "print('Train:', len(train_df), 'Val:', len(val_df), 'Test:', len(test_df))\n",
    "\n",
    "def show_split_stats(name, split_df):\n",
    "    vc = split_df['label'].value_counts(normalize=True).sort_index()\n",
    "    print(f'-- {name} label distribution (fraction) --')\n",
    "    print(vc)\n",
    "\n",
    "show_split_stats('train', train_df)\n",
    "show_split_stats('val', val_df)\n",
    "show_split_stats('test', test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792c707",
   "metadata": {},
   "source": [
    "## Cell 7 — Label mapping\n",
    "Creates a stable `label2id` and `id2label`.\n",
    "\n",
    "**Expected output:** the mapping table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a3644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sorted = sorted(df['label'].unique().tolist())\n",
    "label2id = {lbl: i for i, lbl in enumerate(labels_sorted)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "print('Labels:', labels_sorted)\n",
    "print('label2id:', label2id)\n",
    "\n",
    "def encode_labels(split_df):\n",
    "    out = split_df.copy()\n",
    "    out['label_id'] = out['label'].map(label2id)\n",
    "    if out['label_id'].isna().any():\n",
    "        raise ValueError('Found unknown labels during encoding.')\n",
    "    out['label_id'] = out['label_id'].astype(int)\n",
    "    return out\n",
    "\n",
    "train_df = encode_labels(train_df)\n",
    "val_df = encode_labels(val_df)\n",
    "test_df = encode_labels(test_df)\n",
    "\n",
    "display(pd.DataFrame({'label': labels_sorted, 'id': [label2id[x] for x in labels_sorted]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74e2c6",
   "metadata": {},
   "source": [
    "## Cell 8 — Metrics + plotting helpers\n",
    "**Expected output:** none (helpers only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f0271d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    weighted_f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "    }\n",
    "\n",
    "def save_json(path, obj):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def plot_and_save_confusion_matrix(y_true, y_pred, labels, out_path):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')\n",
    "    plt.title('Confusion Matrix (Test)')\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def plot_and_save_learning_curves(trainer_state_log_history, out_path):\n",
    "    steps = []\n",
    "    train_loss = []\n",
    "    eval_steps = []\n",
    "    eval_loss = []\n",
    "\n",
    "    for row in trainer_state_log_history:\n",
    "        if 'loss' in row and 'eval_loss' not in row:\n",
    "            if 'step' in row:\n",
    "                steps.append(row['step'])\n",
    "                train_loss.append(row['loss'])\n",
    "        if 'eval_loss' in row:\n",
    "            if 'step' in row:\n",
    "                eval_steps.append(row['step'])\n",
    "                eval_loss.append(row['eval_loss'])\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    if steps:\n",
    "        plt.plot(steps, train_loss, label='train_loss')\n",
    "    if eval_steps:\n",
    "        plt.plot(eval_steps, eval_loss, label='eval_loss')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ecfb3",
   "metadata": {},
   "source": [
    "## Cell 9 — Tokenization + dataset preparation helper\n",
    "We tokenize separately per model (because tokenizers differ across model families).\n",
    "\n",
    "**Expected output:** none (helpers only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a28dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_hf_dataset(split_df):\n",
    "    from datasets import Dataset\n",
    "    return Dataset.from_pandas(\n",
    "        split_df[['text', 'label_id']].rename(columns={'label_id': 'labels'}).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "train_hf = df_to_hf_dataset(train_df)\n",
    "val_hf = df_to_hf_dataset(val_df)\n",
    "test_hf = df_to_hf_dataset(test_df)\n",
    "\n",
    "def make_tokenized_datasets(model_ckpt: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt, use_fast=True)\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(batch['text'], truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    train_tok = train_hf.map(tokenize_batch, batched=True)\n",
    "    val_tok = val_hf.map(tokenize_batch, batched=True)\n",
    "    test_tok = test_hf.map(tokenize_batch, batched=True)\n",
    "\n",
    "    cols_to_keep = ['input_ids', 'attention_mask', 'labels']\n",
    "    train_tok = train_tok.remove_columns([c for c in train_tok.column_names if c not in cols_to_keep])\n",
    "    val_tok = val_tok.remove_columns([c for c in val_tok.column_names if c not in cols_to_keep])\n",
    "    test_tok = test_tok.remove_columns([c for c in test_tok.column_names if c not in cols_to_keep])\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    return tokenizer, data_collator, train_tok, val_tok, test_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efbe2e",
   "metadata": {},
   "source": [
    "## Cell 10 — Training helper (Trainer)\n",
    "This function runs a training configuration and saves artifacts into a per-run folder.\n",
    "\n",
    "**Expected output:** training logs, evaluation metrics, and saved plots/files under `runs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "843bf4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import shutil\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class RunResult:\n",
    "    run_name: str\n",
    "    model_ckpt: str\n",
    "    output_dir: str\n",
    "    eval_metrics: dict\n",
    "    test_metrics: dict\n",
    "\n",
    "def run_training(run_cfg: dict) -> RunResult:\n",
    "    run_name = run_cfg[\"run_name\"]\n",
    "    model_ckpt = run_cfg[\"model_ckpt\"]\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "    out_dir = os.path.join(BASE_OUTPUT_DIR, f\"{timestamp}_{run_name}_seed{SEED}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer, data_collator, train_tok, val_tok, test_tok = make_tokenized_datasets(model_ckpt)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_ckpt,\n",
    "        num_labels=len(labels_sorted),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    if run_cfg.get(\"dropout\") is not None:\n",
    "        d = float(run_cfg[\"dropout\"])\n",
    "        for attr in (\n",
    "            \"hidden_dropout_prob\",\n",
    "            \"attention_probs_dropout_prob\",\n",
    "            \"dropout\",\n",
    "            \"attention_dropout\",\n",
    "            \"classifier_dropout\",\n",
    "        ):\n",
    "            if hasattr(config, attr):\n",
    "                setattr(config, attr, d)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config)\n",
    "\n",
    "    trainer_dir = os.path.join(out_dir, \"trainer\")\n",
    "    os.makedirs(trainer_dir, exist_ok=True)\n",
    "\n",
    "    save_strategy = str(run_cfg.get(\"save_strategy\", \"epoch\"))\n",
    "\n",
    "    ta_kwargs = dict(\n",
    "        output_dir=trainer_dir,\n",
    "        learning_rate=LR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=float(run_cfg.get(\"weight_decay\", 0.0)),\n",
    "        save_strategy=save_strategy,\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        greater_is_better=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        save_total_limit=1,\n",
    "        report_to=[],\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    load_best = run_cfg.get(\"load_best_model_at_end\", None)\n",
    "    if load_best is None:\n",
    "        load_best = bool(run_cfg.get(\"use_early_stopping\", False)) and save_strategy != \"no\"\n",
    "    ta_kwargs[\"load_best_model_at_end\"] = bool(load_best)\n",
    "\n",
    "    ta_sig = inspect.signature(TrainingArguments.__init__)\n",
    "    if \"eval_strategy\" in ta_sig.parameters:\n",
    "        ta_kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "    else:\n",
    "        ta_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "\n",
    "    if \"save_only_model\" in ta_sig.parameters:\n",
    "        ta_kwargs[\"save_only_model\"] = bool(run_cfg.get(\"save_only_model\", True))\n",
    "    if \"save_safetensors\" in ta_sig.parameters:\n",
    "        ta_kwargs[\"save_safetensors\"] = True\n",
    "\n",
    "    args = TrainingArguments(**ta_kwargs)\n",
    "\n",
    "    callbacks = []\n",
    "    if run_cfg.get(\"use_early_stopping\", False):\n",
    "        callbacks.append(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "\n",
    "    trainer_kwargs = dict(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    trainer_sig = inspect.signature(Trainer.__init__)\n",
    "    if \"tokenizer\" in trainer_sig.parameters:\n",
    "        trainer_kwargs[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    trainer = Trainer(**trainer_kwargs)\n",
    "\n",
    "    t0 = time.time()\n",
    "    _train_output = trainer.train()\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    eval_metrics = trainer.evaluate()\n",
    "\n",
    "    test_pred = trainer.predict(test_tok)\n",
    "    test_logits = test_pred.predictions\n",
    "    test_labels = test_pred.label_ids\n",
    "    test_preds = np.argmax(test_logits, axis=1)\n",
    "\n",
    "    test_metrics = {\n",
    "        \"accuracy\": float(accuracy_score(test_labels, test_preds)),\n",
    "        \"macro_f1\": float(f1_score(test_labels, test_preds, average=\"macro\")),\n",
    "        \"weighted_f1\": float(f1_score(test_labels, test_preds, average=\"weighted\")),\n",
    "        \"classification_report\": classification_report(\n",
    "            test_labels,\n",
    "            test_preds,\n",
    "            target_names=labels_sorted,\n",
    "            output_dict=True,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    run_info = {\n",
    "        \"run_name\": run_name,\n",
    "        \"model_ckpt\": model_ckpt,\n",
    "        \"seed\": SEED,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LR,\n",
    "        \"weight_decay\": float(run_cfg.get(\"weight_decay\", 0.0)),\n",
    "        \"use_early_stopping\": bool(run_cfg.get(\"use_early_stopping\", False)),\n",
    "        \"dropout\": run_cfg.get(\"dropout\", None),\n",
    "        \"train_time_sec\": float(train_time),\n",
    "        \"label2id\": label2id,\n",
    "    }\n",
    "    save_json(os.path.join(out_dir, \"config.json\"), run_info)\n",
    "    save_json(os.path.join(out_dir, \"eval_metrics.json\"), eval_metrics)\n",
    "    save_json(os.path.join(out_dir, \"test_metrics.json\"), test_metrics)\n",
    "\n",
    "    if bool(run_cfg.get(\"save_predictions\", True)):\n",
    "        pred_df = test_df[[\"text\", \"label\", \"label_id\"]].copy()\n",
    "        pred_df[\"pred_id\"] = test_preds\n",
    "        pred_df[\"pred_label\"] = pred_df[\"pred_id\"].map(id2label)\n",
    "        pred_df.to_csv(os.path.join(out_dir, \"test_predictions.csv\"), index=False, encoding=\"utf-8\")\n",
    "\n",
    "    if bool(run_cfg.get(\"save_plots\", True)):\n",
    "        plot_and_save_confusion_matrix(\n",
    "            test_labels, test_preds, labels_sorted, os.path.join(out_dir, \"confusion_matrix.png\")\n",
    "        )\n",
    "        plot_and_save_learning_curves(\n",
    "            trainer.state.log_history, os.path.join(out_dir, \"learning_curves.png\")\n",
    "        )\n",
    "\n",
    "    if bool(run_cfg.get(\"save_final_model\", True)):\n",
    "        model_dir = os.path.join(out_dir, \"model\")\n",
    "        trainer.save_model(model_dir)\n",
    "        tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "    if bool(run_cfg.get(\"cleanup_trainer_dir\", False)):\n",
    "        try:\n",
    "            shutil.rmtree(trainer_dir, ignore_errors=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    cleaned_run_dir = False\n",
    "    if bool(run_cfg.get(\"cleanup_run_dir\", False)):\n",
    "        try:\n",
    "            shutil.rmtree(out_dir, ignore_errors=True)\n",
    "            cleaned_run_dir = True\n",
    "        except Exception:\n",
    "            cleaned_run_dir = False\n",
    "\n",
    "    if cleaned_run_dir:\n",
    "        print(\"Sweep mode: deleted run folder:\", out_dir)\n",
    "    else:\n",
    "        print(\"Saved run artifacts to:\", out_dir)\n",
    "\n",
    "    return RunResult(\n",
    "        run_name=run_name,\n",
    "        model_ckpt=model_ckpt,\n",
    "        output_dir=out_dir,\n",
    "        eval_metrics=eval_metrics,\n",
    "        test_metrics=test_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf2139",
   "metadata": {},
   "source": [
    "## Cell 11 — Run all experiments\n",
    "This will train and evaluate each run in `RUNS`.\n",
    "\n",
    "**Expected output:** training logs per run + saved artifacts under `runs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b92bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for cfg in RUNS:\n",
    "    print('\\n' + '='*90)\n",
    "    print('Starting:', cfg['run_name'], 'Model:', cfg['model_ckpt'])\n",
    "    print('='*90)\n",
    "    res = run_training(cfg)\n",
    "    results.append(res)\n",
    "\n",
    "print('\\nDone. Completed runs:', [r.run_name for r in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac446ab",
   "metadata": {},
   "source": [
    "## Cell 12 — Summarize results\n",
    "We summarize eval/test metrics across runs in a table.\n",
    "\n",
    "**Expected output:** a table comparing `accuracy`, `macro_f1`, and `weighted_f1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "for r in results:\n",
    "    row = {\n",
    "        'run_name': r.run_name,\n",
    "        'model': r.model_ckpt,\n",
    "        'eval_accuracy': float(r.eval_metrics.get('eval_accuracy', np.nan)),\n",
    "        'eval_macro_f1': float(r.eval_metrics.get('eval_macro_f1', np.nan)),\n",
    "        'eval_weighted_f1': float(r.eval_metrics.get('eval_weighted_f1', np.nan)),\n",
    "        'test_accuracy': float(r.test_metrics.get('accuracy', np.nan)),\n",
    "        'test_macro_f1': float(r.test_metrics.get('macro_f1', np.nan)),\n",
    "        'test_weighted_f1': float(r.test_metrics.get('weighted_f1', np.nan)),\n",
    "        'output_dir': r.output_dir,\n",
    "    }\n",
    "    summary_rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(by='test_macro_f1', ascending=False)\n",
    "display(summary_df)\n",
    "\n",
    "summary_path = os.path.join(BASE_OUTPUT_DIR, 'summary_latest.json')\n",
    "save_json(summary_path, summary_rows)\n",
    "print('Saved summary to:', summary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe8d6d",
   "metadata": {},
   "source": [
    "## Cell 13 — Hyperparameter sweep runner\n",
    "This runs a **small grid search** over a few values of `(dropout, weight_decay, learning_rate)` and repeats each setting across multiple seeds.\n",
    "\n",
    "**Selection uses VALIDATION macro-F1** (not test), so we keep test as an unbiased final evaluation.\n",
    "\n",
    "**Tip:** keep this small. Each trial is a full fine-tune run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be092410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "# ----- Sweep hyperparameter grid -----\n",
    "# Total trials = len(SWEEP_DROPOUT) * len(SWEEP_WEIGHT_DECAY) * len(SWEEP_LR) * len(SWEEP_SEEDS)\n",
    "SWEEP_DROPOUT = [0.0, 0.1]\n",
    "SWEEP_WEIGHT_DECAY = [0.0, 0.001]\n",
    "SWEEP_LR = [1e-5, 2e-5]\n",
    "SWEEP_SEEDS = [13, 42, 123]  # 3 seeds minimum for stability\n",
    "\n",
    "MAX_TRIALS = 50  # safety guard\n",
    "\n",
    "# Disk safety\n",
    "SWEEP_DELETE_OLD_SWEEP_RUNS = True\n",
    "SWEEP_MIN_FREE_GB = 1.0\n",
    "\n",
    "# For sweeps, skip heavy artifacts per trial\n",
    "SWEEP_SAVE_FINAL_MODEL = False\n",
    "SWEEP_SAVE_PREDICTIONS = False\n",
    "SWEEP_SAVE_PLOTS = False\n",
    "SWEEP_CLEANUP_TRAINER_DIR = True\n",
    "SWEEP_CLEANUP_RUN_DIR = True\n",
    "\n",
    "def _disk_free_gb(path: str) -> float:\n",
    "    du = shutil.disk_usage(path)\n",
    "    return float(du.free) / (1024 ** 3)\n",
    "\n",
    "def _delete_old_sweep_runs(base_dir: str) -> int:\n",
    "    if not os.path.isdir(base_dir):\n",
    "        return 0\n",
    "    deleted = 0\n",
    "    for name in os.listdir(base_dir):\n",
    "        full = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(full) and '_S_' in name:\n",
    "            shutil.rmtree(full, ignore_errors=True)\n",
    "            deleted += 1\n",
    "    return deleted\n",
    "\n",
    "def _fmt_float(x) -> str:\n",
    "    if x is None:\n",
    "        return 'none'\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return str(int(x))\n",
    "    if math.isclose(float(x), 0.0):\n",
    "        return '0'\n",
    "    return f'{float(x):g}'.replace('.', 'p')\n",
    "\n",
    "def _make_sweep_run_name(dropout, weight_decay, lr) -> str:\n",
    "    return f\"S_bert_do{_fmt_float(dropout)}_wd{_fmt_float(weight_decay)}_lr{_fmt_float(lr)}\"\n",
    "\n",
    "base_cfg = {\n",
    "    'run_name': 'SWEEP',\n",
    "    'model_ckpt': 'bert-base-uncased',\n",
    "    'use_early_stopping': True,\n",
    "    'weight_decay': 0.0,\n",
    "    'dropout': None,\n",
    "    'save_strategy': 'no',\n",
    "    'load_best_model_at_end': False,\n",
    "    'save_only_model': True,\n",
    "    'save_final_model': SWEEP_SAVE_FINAL_MODEL,\n",
    "    'save_predictions': SWEEP_SAVE_PREDICTIONS,\n",
    "    'save_plots': SWEEP_SAVE_PLOTS,\n",
    "    'cleanup_trainer_dir': SWEEP_CLEANUP_TRAINER_DIR,\n",
    "    'cleanup_run_dir': SWEEP_CLEANUP_RUN_DIR,\n",
    "}\n",
    "\n",
    "grid = list(itertools.product(SWEEP_DROPOUT, SWEEP_WEIGHT_DECAY, SWEEP_LR, SWEEP_SEEDS))\n",
    "\n",
    "try:\n",
    "    free_gb = _disk_free_gb(BASE_OUTPUT_DIR)\n",
    "    print(f'Free disk space: {free_gb:.2f} GB')\n",
    "    if SWEEP_DELETE_OLD_SWEEP_RUNS and free_gb < SWEEP_MIN_FREE_GB:\n",
    "        n_del = _delete_old_sweep_runs(BASE_OUTPUT_DIR)\n",
    "        print(f'Deleted {n_del} old sweep folders. Free now: {_disk_free_gb(BASE_OUTPUT_DIR):.2f} GB')\n",
    "except Exception as e:\n",
    "    print('Disk check skipped:', repr(e))\n",
    "\n",
    "print(f'Planned trials: {len(grid)}')\n",
    "if len(grid) > MAX_TRIALS:\n",
    "    raise ValueError(f'Sweep too large ({len(grid)} > {MAX_TRIALS}). Reduce grid or raise MAX_TRIALS.')\n",
    "\n",
    "_orig_seed = SEED\n",
    "_orig_lr = LR\n",
    "\n",
    "sweep_rows = []\n",
    "sweep_results = []\n",
    "\n",
    "for dropout, weight_decay, lr, seed in grid:\n",
    "    SEED = int(seed)\n",
    "    LR = float(lr)\n",
    "    set_seed(SEED)\n",
    "\n",
    "    trial_cfg = dict(base_cfg)\n",
    "    trial_cfg['run_name'] = _make_sweep_run_name(dropout, weight_decay, lr)\n",
    "    trial_cfg['dropout'] = float(dropout)\n",
    "    trial_cfg['weight_decay'] = float(weight_decay)\n",
    "\n",
    "    print('\\n' + '-'*90)\n",
    "    print('Trial:', trial_cfg['run_name'], 'seed=', SEED)\n",
    "    print('Params:', {'dropout': dropout, 'weight_decay': weight_decay, 'lr': lr})\n",
    "    print('-'*90)\n",
    "\n",
    "    rr = run_training(trial_cfg)\n",
    "    sweep_results.append(rr)\n",
    "\n",
    "    # --- CHANGED: record VALIDATION metrics for selection (not test) ---\n",
    "    sweep_rows.append({\n",
    "        'run_name': rr.run_name,\n",
    "        'model': rr.model_ckpt,\n",
    "        'dropout': float(dropout),\n",
    "        'weight_decay': float(weight_decay),\n",
    "        'lr': float(lr),\n",
    "        'seed': int(seed),\n",
    "        # Selection metrics (validation)\n",
    "        'val_accuracy': float(rr.eval_metrics.get('eval_accuracy', np.nan)),\n",
    "        'val_macro_f1': float(rr.eval_metrics.get('eval_macro_f1', np.nan)),\n",
    "        'val_weighted_f1': float(rr.eval_metrics.get('eval_weighted_f1', np.nan)),\n",
    "        # Also record test for reporting (but NOT used to pick best)\n",
    "        'test_accuracy': float(rr.test_metrics.get('accuracy', np.nan)),\n",
    "        'test_macro_f1': float(rr.test_metrics.get('macro_f1', np.nan)),\n",
    "        'test_weighted_f1': float(rr.test_metrics.get('weighted_f1', np.nan)),\n",
    "        'output_dir': None if SWEEP_CLEANUP_RUN_DIR else rr.output_dir,\n",
    "    })\n",
    "\n",
    "SEED = _orig_seed\n",
    "LR = _orig_lr\n",
    "set_seed(SEED)\n",
    "\n",
    "sweep_df = pd.DataFrame(sweep_rows)\n",
    "display(sweep_df.sort_values(by='val_macro_f1', ascending=False))\n",
    "\n",
    "# --- Aggregate across seeds using VALIDATION macro-F1 ---\n",
    "group_cols = ['dropout', 'weight_decay', 'lr']\n",
    "agg_df = (\n",
    "    sweep_df.groupby(group_cols)\n",
    "    .agg(\n",
    "        n_trials=('val_macro_f1', 'count'),\n",
    "        mean_val_macro_f1=('val_macro_f1', 'mean'),\n",
    "        std_val_macro_f1=('val_macro_f1', 'std'),\n",
    "        mean_val_accuracy=('val_accuracy', 'mean'),\n",
    "        # Also aggregate test for later reference\n",
    "        mean_test_macro_f1=('test_macro_f1', 'mean'),\n",
    "        std_test_macro_f1=('test_macro_f1', 'std'),\n",
    "    )\n",
    "    .sort_values(by=['mean_val_macro_f1', 'mean_val_accuracy'], ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "display(agg_df)\n",
    "\n",
    "best = agg_df.iloc[0].to_dict() if len(agg_df) else None\n",
    "print('Best setting by mean VALIDATION macro-F1:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05450b",
   "metadata": {},
   "source": [
    "## Final best run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "258b6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires: run Cell 27 first so `best` is available.\n",
    "\n",
    "final_cfg = {\n",
    "    'run_name': 'FINAL_best_from_sweep',\n",
    "    'model_ckpt': 'bert-base-uncased',\n",
    "    'use_early_stopping': True,\n",
    "    'dropout': float(0.1),\n",
    "    'weight_decay': float(0.001),\n",
    "    'save_predictions': True,\n",
    "    'save_plots': True,\n",
    "    'save_final_model': True,\n",
    "    'save_strategy': 'epoch',\n",
    "    'load_best_model_at_end': True,\n",
    "    'save_only_model': True,\n",
    "    'cleanup_trainer_dir': True,\n",
    "    'cleanup_run_dir': False,\n",
    "}\n",
    "\n",
    "_orig_seed = SEED\n",
    "_orig_lr = LR\n",
    "SEED = 42\n",
    "LR = float(2e-5)\n",
    "set_seed(SEED)\n",
    "\n",
    "print('Final run config:', {'dropout': final_cfg['dropout'], 'weight_decay': final_cfg['weight_decay']}, 'LR=', LR, 'SEED=', SEED)\n",
    "final_result = run_training(final_cfg)\n",
    "\n",
    "# Restore globals\n",
    "SEED = _orig_seed\n",
    "LR = _orig_lr\n",
    "set_seed(SEED)\n",
    "\n",
    "print('\\n=== Final Test Metrics (real-world estimate) ===')\n",
    "print('test_accuracy:', final_result.test_metrics.get('accuracy'))\n",
    "print('test_macro_f1:', final_result.test_metrics.get('macro_f1'))\n",
    "print('test_weighted_f1:', final_result.test_metrics.get('weighted_f1'))\n",
    "print('\\nFinal run saved at:', final_result.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e57f964",
   "metadata": {},
   "source": [
    "## Re-run FINAL config across 3 seeds (report mean/std)\n",
    "This runs the *same* final hyperparameters across multiple **training seeds** on the **same fixed split** (controlled by `SPLIT_SEED`).\n",
    "\n",
    "Report the stability as mean/std of **test macro-F1** across seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a36372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New — run final config across multiple training seeds\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "FINAL_SEEDS = [13, 42, 123]  # adjust if you want more\n",
    "\n",
    "# Make sure final_cfg exists (Cell 28 defines it)\n",
    "if 'final_cfg' not in globals():\n",
    "    raise RuntimeError('final_cfg not found. Run the Final config cell first.')\n",
    "\n",
    "_orig_seed = SEED\n",
    "_orig_lr = LR\n",
    "\n",
    "rows = []\n",
    "results_multi = []\n",
    "for s in FINAL_SEEDS:\n",
    "    SEED = int(s)\n",
    "    LR = float(2e-5)  # keep fixed for fair seed comparison\n",
    "    set_seed(SEED)\n",
    "\n",
    "    cfg = dict(final_cfg)\n",
    "    cfg['run_name'] = f\"FINAL_seed{SEED}\"\n",
    "    print('\\n' + '='*90)\n",
    "    print('Running:', cfg['run_name'], 'SPLIT_SEED=', SPLIT_SEED, 'TRAIN_SEED=', SEED, 'LR=', LR)\n",
    "    print('='*90)\n",
    "    rr = run_training(cfg)\n",
    "    results_multi.append(rr)\n",
    "    rows.append({\n",
    "        'seed': SEED,\n",
    "        'test_accuracy': float(rr.test_metrics.get('accuracy', math.nan)),\n",
    "        'test_macro_f1': float(rr.test_metrics.get('macro_f1', math.nan)),\n",
    "        'test_weighted_f1': float(rr.test_metrics.get('weighted_f1', math.nan)),\n",
    "        'val_macro_f1': float(rr.eval_metrics.get('eval_macro_f1', math.nan)),\n",
    "        'output_dir': rr.output_dir,\n",
    "    })\n",
    "\n",
    "# Restore globals\n",
    "SEED = _orig_seed\n",
    "LR = _orig_lr\n",
    "set_seed(SEED)\n",
    "\n",
    "multi_df = pd.DataFrame(rows).sort_values('seed')\n",
    "display(multi_df)\n",
    "\n",
    "print('\\n=== Stability summary (across seeds) ===')\n",
    "print('macro-F1 mean:', float(multi_df['test_macro_f1'].mean()))\n",
    "print('macro-F1 std :', float(multi_df['test_macro_f1'].std(ddof=1)))\n",
    "print('acc mean    :', float(multi_df['test_accuracy'].mean()))\n",
    "print('acc std     :', float(multi_df['test_accuracy'].std(ddof=1)))\n",
    "\n",
    "out_path = Path(BASE_OUTPUT_DIR) / f\"final_multiseed_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.csv\"\n",
    "multi_df.to_csv(out_path, index=False, encoding='utf-8')\n",
    "print('Saved multiseed summary to:', str(out_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56bf5e8",
   "metadata": {},
   "source": [
    "##  Real life test: run predictions on your own texts/CSVs\n",
    "Use this when you have new DREAMS-like inputs (captions or dream narratives) that are **not** from the training dataset.\n",
    "\n",
    "It generates a `predictions.csv` with predicted CHIME label + confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# A) External data source\n",
    "USE_HF_EXTERNAL_DATASET = True\n",
    "HF_EXTERNAL_DATASET_NAME = 'ashh007/CHIME_external_evaluation'\n",
    "HF_EXTERNAL_SPLIT = 'train'  # change if your dataset uses a different split\n",
    "HF_TEXT_COLUMN_CANDIDATES = ['text', 'CAPTIONS', 'captions', 'caption', 'sentence', 'content']\n",
    "HF_LABEL_COLUMN_CANDIDATES = ['label', 'labels', 'LABEL', 'Labels', 'category', 'class', 'target']\n",
    "HF_EXTERNAL_FILENAME = 'external_eval.csv'  # used by fallback loader (raw file download)\n",
    "\n",
    "# If you want local CSV instead, set USE_HF_EXTERNAL_DATASET=False\n",
    "EXTERNAL_CSV = Path('ml_experiments_anish') / 'external_eval.csv'\n",
    "CSV_TEXT_COLUMN = 'text'\n",
    "CSV_LABEL_COLUMN = 'labels' \n",
    "\n",
    "# B) Model source\n",
    "# IMPORTANT: you must have a trained model accessible either locally OR on Hugging Face Hub.\n",
    "USE_HF_MODEL = False\n",
    "HF_MODEL_ID = 'PUT_YOUR_USERNAME/PUT_YOUR_MODEL'  # if USE_HF_MODEL=True, set this to your fine-tuned model repo\n",
    "LOCAL_MODEL_DIR = None  # set to a specific .../model folder if you have one locally\n",
    "\n",
    "MAX_LEN = int(globals().get('MAX_LENGTH', 128))\n",
    "\n",
    "def _pick_first_existing(columns: list[str], candidates: list[str]) -> str | None:\n",
    "    colset = set(columns)\n",
    "    for c in candidates:\n",
    "        if c in colset:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _load_hf_external_fallback(repo_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Fallback for HF dataset repos that contain a CSV with unquoted commas in the text field.\n",
    "\n",
    "    The HF `datasets` CSV loader uses pandas' C-engine which expects proper quoting. If the\n",
    "    file is `text,label` but text contains commas and isn't quoted, it will fail. This loader\n",
    "    downloads the raw file and splits each row on the *last* comma, assuming the label field\n",
    "    never contains commas.\n",
    "    \"\"\"\n",
    "    from huggingface_hub import hf_hub_download\n",
    "\n",
    "    csv_path = hf_hub_download(\n",
    "        repo_id=repo_id, repo_type='dataset', filename=HF_EXTERNAL_FILENAME\n",
    "    )\n",
    "    rows: list[dict[str, str]] = []\n",
    "    bad = 0\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        header = f.readline().strip()\n",
    "        # Expect 2 columns; use last header token as label name when possible\n",
    "        header_parts = [h.strip() for h in header.split(',') if h.strip()]\n",
    "        label_name = header_parts[-1] if len(header_parts) >= 2 else 'label'\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            if ',' not in line:\n",
    "                bad += 1\n",
    "                continue\n",
    "            text_part, label_part = line.rsplit(',', 1)\n",
    "            text_part = text_part.strip()\n",
    "            label_part = label_part.strip()\n",
    "            # Strip surrounding quotes if present (common when partially cleaned)\n",
    "            if len(text_part) >= 2 and text_part[0] == '\"' and text_part[-1] == '\"':\n",
    "                text_part = text_part[1:-1].replace('\"\"', '\"')\n",
    "            rows.append({'text': text_part, 'label': label_part})\n",
    "    if bad:\n",
    "        print(f'Fallback parser skipped {bad} malformed line(s).')\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f'Fallback parser produced 0 rows from {repo_id}/{HF_EXTERNAL_FILENAME}')\n",
    "    # Keep a copy of original label column name if you want it\n",
    "    if label_name != 'label':\n",
    "        df = df.rename(columns={'label': label_name})\n",
    "        # also provide normalized 'label' used by downstream code\n",
    "        df['label'] = df[label_name].astype(str)\n",
    "    else:\n",
    "        df['label'] = df['label'].astype(str)\n",
    "    df['text'] = df['text'].astype(str).fillna('')\n",
    "    return df\n",
    "\n",
    "def _load_external_df() -> pd.DataFrame:\n",
    "    if USE_HF_EXTERNAL_DATASET:\n",
    "        if HF_EXTERNAL_DATASET_NAME.startswith('PUT_YOUR_'):\n",
    "            raise ValueError('Set HF_EXTERNAL_DATASET_NAME to your dataset id, e.g. \"yourname/external-eval\"')\n",
    "        try:\n",
    "            ds = load_dataset(HF_EXTERNAL_DATASET_NAME)\n",
    "            split = HF_EXTERNAL_SPLIT if HF_EXTERNAL_SPLIT in ds else list(ds.keys())[0]\n",
    "            base = ds[split]\n",
    "            df_ext = base.to_pandas()\n",
    "            tcol = _pick_first_existing(list(df_ext.columns), HF_TEXT_COLUMN_CANDIDATES)\n",
    "            lcol = _pick_first_existing(list(df_ext.columns), HF_LABEL_COLUMN_CANDIDATES)\n",
    "            if tcol is None:\n",
    "                raise ValueError(f'Could not find a text column in HF dataset. Found: {list(df_ext.columns)}')\n",
    "            out = df_ext.copy()\n",
    "            out = out.rename(columns={tcol: 'text', **({lcol: 'label'} if lcol else {})})\n",
    "            out['text'] = out['text'].astype(str).fillna('')\n",
    "            if 'label' in out.columns:\n",
    "                out['label'] = out['label'].astype(str)\n",
    "            return out\n",
    "        except Exception as e:\n",
    "            # This is commonly caused by unquoted commas in the text field of a CSV file\n",
    "            msg = str(e)\n",
    "            print('HF external dataset load failed; using fallback raw CSV parser.')\n",
    "            print('Original error (truncated):', msg[:300])\n",
    "            return _load_hf_external_fallback(HF_EXTERNAL_DATASET_NAME)\n",
    "    else:\n",
    "        if not EXTERNAL_CSV.exists():\n",
    "            raise FileNotFoundError(f'External CSV not found at: {EXTERNAL_CSV.resolve()}')\n",
    "        # Use python engine to be tolerant of commas in text if the file isn't perfectly quoted\n",
    "        df_ext = pd.read_csv(EXTERNAL_CSV, engine='python')\n",
    "        if CSV_TEXT_COLUMN not in df_ext.columns:\n",
    "            raise ValueError(f\"External CSV must contain column '{CSV_TEXT_COLUMN}'. Found: {list(df_ext.columns)}\")\n",
    "        out = df_ext.copy()\n",
    "        out = out.rename(columns={CSV_TEXT_COLUMN: 'text'})\n",
    "        out['text'] = out['text'].astype(str).fillna('')\n",
    "        if CSV_LABEL_COLUMN and (CSV_LABEL_COLUMN in out.columns):\n",
    "            out = out.rename(columns={CSV_LABEL_COLUMN: 'label'})\n",
    "            out['label'] = out['label'].astype(str)\n",
    "        return out\n",
    "\n",
    "def _resolve_model_dir() -> Path:\n",
    "    # 1) Hugging Face model (best if you trained on Colab and pushed the model)\n",
    "    if USE_HF_MODEL:\n",
    "        if HF_MODEL_ID.startswith('PUT_YOUR_'):\n",
    "            raise ValueError('Set HF_MODEL_ID to your fine-tuned model id, e.g. \"yourname/chime-bert\"')\n",
    "        return Path(HF_MODEL_ID)  # transformers can load from hub id as a string\n",
    "\n",
    "    # 2) Explicit local model dir (if you copied from Colab or trained locally)\n",
    "    if LOCAL_MODEL_DIR is not None:\n",
    "        p = Path(LOCAL_MODEL_DIR)\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise FileNotFoundError(f'LOCAL_MODEL_DIR does not exist: {p}')\n",
    "\n",
    "    # 3) Try to use final_result if present in this kernel\n",
    "    if 'final_result' in globals() and getattr(final_result, 'output_dir', None):\n",
    "        cand = Path(final_result.output_dir) / 'model'\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "\n",
    "    # 4) Auto-search common local runs folder\n",
    "    default_runs = Path('ml_experiments_anish') / 'experiment1_chime_text_overfitting' / 'runs'\n",
    "    if default_runs.exists():\n",
    "        model_dirs = sorted(default_runs.glob('**/model'), key=lambda p: p.parent.name)\n",
    "        if model_dirs:\n",
    "            return model_dirs[-1]\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        'No trained model found. If you trained on Colab, either (a) push your model to Hugging Face Hub and set USE_HF_MODEL=True, '\n",
    "        'or (b) copy the saved run folder locally and set LOCAL_MODEL_DIR to that folder.'\n",
    "    )\n",
    "\n",
    "def predict_texts(texts: list[str], model_ref, batch_size: int = 16):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(model_ref), use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(str(model_ref)).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    id2label = {int(k): v for k, v in getattr(model.config, 'id2label', {}).items()}\n",
    "    label2id = {k: int(v) for k, v in getattr(model.config, 'label2id', {}).items()}\n",
    "\n",
    "    all_pred_ids: list[int] = []\n",
    "    all_conf: list[float] = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            enc = tokenizer(batch, truncation=True, max_length=MAX_LEN, padding=True, return_tensors='pt')\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            conf, pred = torch.max(probs, dim=-1)\n",
    "            all_pred_ids.extend([int(x) for x in pred.cpu().tolist()])\n",
    "            all_conf.extend([float(x) for x in conf.cpu().tolist()])\n",
    "    return all_pred_ids, all_conf, id2label, label2id\n",
    "\n",
    "# Run external eval\n",
    "ext = _load_external_df()\n",
    "model_ref = _resolve_model_dir()\n",
    "print('External rows:', len(ext))\n",
    "print('Model source:', ('HF Hub' if USE_HF_MODEL else 'local/search'))\n",
    "print('Model ref:', str(model_ref))\n",
    "\n",
    "pred_ids, conf, id2label, label2id = predict_texts(ext['text'].tolist(), model_ref=model_ref, batch_size=16)\n",
    "ext['pred_id'] = pred_ids\n",
    "ext['pred_label'] = [id2label.get(int(i), str(i)) for i in ext['pred_id'].tolist()]\n",
    "ext['pred_conf'] = conf\n",
    "\n",
    "# Optional scoring if labels available\n",
    "if 'label' in ext.columns:\n",
    "    label_ids = ext['label'].astype(str).map(label2id)\n",
    "    ok = label_ids.notna()\n",
    "    if ok.any():\n",
    "        from sklearn.metrics import accuracy_score, f1_score\n",
    "        y_true = label_ids[ok].astype(int).to_numpy()\n",
    "        y_pred = ext.loc[ok, 'pred_id'].astype(int).to_numpy()\n",
    "        print('External accuracy:', float(accuracy_score(y_true, y_pred)))\n",
    "        print('External macro_f1:', float(f1_score(y_true, y_pred, average='macro')))\n",
    "        print('External weighted_f1:', float(f1_score(y_true, y_pred, average='weighted')))\n",
    "    else:\n",
    "        print('External labels present, but they do not match model labels:', list(label2id.keys()))\n",
    "\n",
    "# Save a local copy for inspection\n",
    "out_path = Path('ml_experiments_anish') / 'external_eval_predictions.csv'\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "ext.to_csv(out_path, index=False, encoding='utf-8')\n",
    "print('Wrote predictions to:', str(out_path.resolve()))\n",
    "display(ext.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
